import os
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from torchvision import transforms
from PIL import Image
import requests
import json
from io import BytesIO
import pickle
from pathlib import Path
import hashlib
import random
from typing import Optional, List, Tuple
from tqdm import tqdm


class CelebrityFacesDataset(Dataset):
    """Celebrity Faces Dataset from Hugging Face with advanced caching"""

    def __init__(self, transform=None, max_images=None, split="train", train_ratio=0.8, 
                 dataset_seed=42, cache_dir="./cache", validate_cache=True):
        self.transform = transform
        self.max_images = max_images
        self.split = split
        self.train_ratio = train_ratio
        self.dataset_seed = dataset_seed
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.validate_cache = validate_cache
        
        # Cache files
        self.urls_cache_file = self.cache_dir / f"celebrity_urls_{max_images}_{dataset_seed}.pkl"
        self.images_cache_dir = self.cache_dir / "images"
        self.images_cache_dir.mkdir(exist_ok=True)
        self.metadata_file = self.cache_dir / f"metadata_{max_images}_{dataset_seed}.pkl"

        # Load or download dataset
        print(f"Initializing celebrity faces dataset ({split} split)...")
        self._load_or_download_dataset()

        # Split into train/validation
        self._split_dataset()

        print(f"Dataset ready: {len(self.image_data)} images for {split} split")

    def _get_image_cache_path(self, url: str) -> Path:
        """Generate cache path for image based on URL hash"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.images_cache_dir / f"{url_hash}.jpg"

    def _validate_cached_image(self, cache_path: Path) -> bool:
        """Validate that cached image is not corrupted"""
        if not cache_path.exists():
            return False
        try:
            img = Image.open(cache_path)
            img.verify()  # Verify image integrity
            return True
        except:
            return False

    def _download_and_cache_image(self, url: str, cache_path: Path) -> Optional[Image.Image]:
        """Download image and save to cache"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # Validate image data
            img = Image.open(BytesIO(response.content)).convert("RGB")
            
            # Save to cache
            img.save(cache_path, "JPEG", quality=95)
            return img
            
        except Exception as e:
            print(f"Error downloading image from {url}: {e}")
            return None

    def _load_or_download_dataset(self):
        """Load dataset from cache or download if needed"""
        
        # Check if we have complete cached data
        if self._load_from_cache():
            print("Successfully loaded dataset from cache")
            return
        
        print("Cache incomplete or invalid, downloading dataset...")
        self._download_and_cache_dataset()

    def _load_from_cache(self) -> bool:
        """Try to load dataset from cache"""
        
        # Check if metadata exists
        if not self.metadata_file.exists():
            return False
        
        try:
            with open(self.metadata_file, 'rb') as f:
                metadata = pickle.load(f)
            
            # Check if cache matches current parameters
            if (metadata.get('max_images') != self.max_images or 
                metadata.get('dataset_seed') != self.dataset_seed):
                print("Cache parameters don't match, need to rebuild")
                return False
            
            # Load URLs
            if not self.urls_cache_file.exists():
                return False
                
            with open(self.urls_cache_file, 'rb') as f:
                self.all_image_urls = pickle.load(f)
            
            # Validate cached images if requested
            if self.validate_cache:
                print("Validating cached images...")
                valid_data = []
                for url in tqdm(self.all_image_urls, desc="Validating cache"):
                    cache_path = self._get_image_cache_path(url)
                    if self._validate_cached_image(cache_path):
                        valid_data.append((url, cache_path))
                    else:
                        # Try to re-download invalid image
                        img = self._download_and_cache_image(url, cache_path)
                        if img:
                            valid_data.append((url, cache_path))
                
                if len(valid_data) < len(self.all_image_urls) * 0.95:  # Allow 5% failure rate
                    print(f"Too many invalid cached images ({len(valid_data)}/{len(self.all_image_urls)})")
                    return False
                
                self.all_image_data = valid_data
            else:
                # Trust cache without validation
                self.all_image_data = [(url, self._get_image_cache_path(url)) 
                                       for url in self.all_image_urls]
            
            print(f"Loaded {len(self.all_image_data)} images from cache")
            return True
            
        except Exception as e:
            print(f"Error loading cache: {e}")
            return False

    def _download_and_cache_dataset(self):
        """Download dataset from HuggingFace and cache images"""
        
        # First, try to load existing URLs if we need to expand the dataset
        existing_urls = []
        if self.urls_cache_file.exists():
            try:
                with open(self.urls_cache_file, 'rb') as f:
                    existing_urls = pickle.load(f)
                print(f"Found {len(existing_urls)} existing URLs in cache")
            except:
                pass
        
        # Fetch URLs from HuggingFace API
        base_url = "https://datasets-server.huggingface.co/rows"
        dataset = "ares1123/celebrity_dataset"
        
        all_urls = []
        offset = 0
        batch_size = 100
        target_count = self.max_images if self.max_images else float('inf')
        
        print("Fetching image URLs from HuggingFace...")
        while len(all_urls) < target_count:
            url = f"{base_url}?dataset={dataset}&config=default&split=train&offset={offset}&length={batch_size}"
            
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if not data.get("rows"):
                    break
                
                for row in data["rows"]:
                    img_url = row["row"]["image"]["src"]
                    all_urls.append(img_url)
                    
                    if len(all_urls) >= target_count:
                        break
                
                offset += batch_size
                
                if len(data["rows"]) < batch_size:
                    break
                    
            except Exception as e:
                print(f"Error fetching URLs at offset {offset}: {e}")
                break
        
        # Sort for deterministic ordering
        all_urls.sort()
        if self.max_images:
            all_urls = all_urls[:self.max_images]
        
        print(f"Fetched {len(all_urls)} URLs, downloading images...")
        
        # Download and cache images
        valid_data = []
        failed_count = 0
        
        for url in tqdm(all_urls, desc="Downloading images"):
            cache_path = self._get_image_cache_path(url)
            
            # Check if already cached and valid
            if self._validate_cached_image(cache_path):
                valid_data.append((url, cache_path))
            else:
                # Download and cache
                img = self._download_and_cache_image(url, cache_path)
                if img:
                    valid_data.append((url, cache_path))
                else:
                    failed_count += 1
                    if failed_count > len(all_urls) * 0.1:  # Stop if >10% failures
                        raise RuntimeError("Too many download failures")
        
        print(f"Successfully downloaded {len(valid_data)} images ({failed_count} failures)")
        
        # Save URLs and metadata
        self.all_image_urls = [url for url, _ in valid_data]
        self.all_image_data = valid_data
        
        with open(self.urls_cache_file, 'wb') as f:
            pickle.dump(self.all_image_urls, f)
        
        metadata = {
            'max_images': self.max_images,
            'dataset_seed': self.dataset_seed,
            'total_images': len(self.all_image_urls)
        }
        with open(self.metadata_file, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"Saved dataset to cache: {len(self.all_image_urls)} images")

    def _split_dataset(self):
        """Split dataset into train and validation sets with fixed seed"""
        
        # Use fixed seed for reproducible splits
        random.seed(self.dataset_seed)
        
        # Shuffle data
        shuffled_data = self.all_image_data.copy()
        random.shuffle(shuffled_data)
        
        # Reset random seed
        random.seed()
        
        n_total = len(shuffled_data)
        n_train = int(n_total * self.train_ratio)
        
        if self.split == "train":
            self.image_data = shuffled_data[:n_train]
        elif self.split == "val" or self.split == "validation":
            self.image_data = shuffled_data[n_train:]
        else:
            raise ValueError(f"Invalid split: {self.split}. Must be 'train' or 'val'")

    def __len__(self):
        return len(self.image_data)

    def __getitem__(self, idx):
        url, cache_path = self.image_data[idx]
        
        try:
            # Load from cache
            image = Image.open(cache_path).convert("RGB")
            
            if self.transform:
                image = self.transform(image)
            
            return image, 0
            
        except Exception as e:
            print(f"Error loading cached image {cache_path}: {e}")
            
            # Try to re-download
            try:
                img = self._download_and_cache_image(url, cache_path)
                if img:
                    if self.transform:
                        img = self.transform(img)
                    return img, 0
            except:
                pass
            
            # Return black image as fallback
            image = Image.new("RGB", (256, 256), (0, 0, 0))
            if self.transform:
                image = self.transform(image)
            return image, 0


def check_hf_dataset():
    """Check if Hugging Face dataset is accessible"""
    try:
        url = "https://datasets-server.huggingface.co/rows?dataset=ares1123%2Fcelebrity_dataset&config=default&split=train&offset=0&length=1"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        return len(data.get("rows", [])) > 0
    except Exception as e:
        print(f"Error checking HF dataset: {e}")
        return False


def get_dataloader(config):
    """Create DataLoader for Celebrity Faces dataset from Hugging Face"""
    
    transform = transforms.Compose([
        transforms.Resize(config["dataset"]["image_size"]),
        transforms.CenterCrop(config["dataset"]["image_size"]),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])

    # Check if HF dataset is accessible
    if not check_hf_dataset():
        raise RuntimeError("Cannot access Hugging Face celebrity dataset")

    # Create dataset with caching
    dataset = CelebrityFacesDataset(
        transform=transform,
        max_images=config["dataset"].get("max_images", None),
        split="train",
        train_ratio=config["dataset"].get("train_ratio", 0.8),
        dataset_seed=config["dataset"].get("dataset_seed", 42),
        cache_dir=config["dataset"].get("cache_dir", "./cache"),
        validate_cache=config["dataset"].get("validate_cache", True)
    )

    # Create dataloader with proper shuffling
    dataloader = DataLoader(
        dataset,
        batch_size=config["dataset"]["batch_size"],
        shuffle=True,  # Shuffle batches during training
        num_workers=config["dataset"]["num_workers"],
        drop_last=True,
        pin_memory=True if config.get("device") == "cuda" else False
    )

    return dataloader


def get_dataloader_ddp(config, rank, world_size, split="train"):
    """Get dataloader with DistributedSampler for DDP
    
    Args:
        config: Configuration dictionary
        rank: Current process rank
        world_size: Total number of processes
        split: 'train' or 'val' for train/validation split
    """
    transform = transforms.Compose([
        transforms.Resize(config["dataset"]["image_size"]),
        transforms.CenterCrop(config["dataset"]["image_size"]),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])

    if config["dataset"]["name"] == "celebrity":
        # Use celebrity dataset from Hugging Face
        max_images = config["dataset"].get("max_images", None)
        train_ratio = config["dataset"].get("train_ratio", 0.8)
        dataset_seed = config["dataset"].get("dataset_seed", 42)
        dataset = CelebrityFacesDataset(
            transform=transform, 
            max_images=max_images, 
            split=split,
            train_ratio=train_ratio,
            dataset_seed=dataset_seed
        )
    else:
        from torchvision.datasets import CelebA
        
        dataset = CelebA(
            root=config["dataset"]["data_dir"],
            split="train" if split == "train" else "valid",
            transform=transform,
            download=False,  # Assume already downloaded
        )

    # Create distributed sampler with deterministic option
    deterministic = config["dataset"].get("deterministic", False)
    # Only shuffle if not in deterministic mode and it's the training set
    should_shuffle = (split == "train") and not deterministic
    
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=should_shuffle)

    # Adjust batch size per GPU
    batch_size_per_gpu = config["dataset"]["batch_size"] // world_size

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size_per_gpu,
        sampler=sampler,
        num_workers=config["dataset"]["num_workers"],
        pin_memory=True,
        drop_last=True,
    )

    return dataloader, sampler